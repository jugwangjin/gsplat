import os
import json
import csv
import argparse
import re

def parse_subdir_name(subdir_name, prefix="bicycle_4_SMALL"):
    """
    Parse a subdirectory name generated by the new training script.
    
    Expected naming pattern:
      prefix_blur{blur}_novel{novel_view}_start{start_sampling_ratio}_target{target_sampling_ratio}
      _sh{distill_sh_lambda}_colors{distill_colors_lambda}_depth{distill_depth_lambda}_xyzs{distill_xyzs_lambda}_quats{distill_quats_lambda}
      _{gradient_key}[_shmult{sh_coeffs_mult}][_depth{depths_mult}]_grow2d{grow_grad2d}
    
    This function extracts:
      - blur (bool)
      - novel_view (bool)
      - start_sampling_ratio (float)
      - target_sampling_ratio (float)
      - distill_sh_lambda, distill_colors_lambda, distill_depth_lambda,
        distill_xyzs_lambda, distill_quats_lambda (floats)
      - gradient_key (str)
      - sh_coeffs_mult (float, optional)
      - depths_mult (float, optional)
      - grow_grad2d (float)
    """
    params = {}
    prefix2 = "bicycle_4_FULL"
    # Remove the prefix if present.
    if subdir_name.startswith(prefix):
        rest = subdir_name[len(prefix):].lstrip('_')
    elif subdir_name.startswith(prefix2):
        rest = subdir_name[len(prefix2):].lstrip('_')
    else:
        rest = subdir_name
    tokens = rest.split('_')
    
    # We expect at least 9 tokens for the mandatory parameters.
    if len(tokens) < 9:
        return params
    
    # Token 0: blur flag, e.g. "blurTrue" or "blurFalse"
    if tokens[0].startswith("blur"):
        blur_val = tokens[0][len("blur"):]
        params["blur"] = True if blur_val.lower() in ("true", "1") else False
    else:
        params["blur"] = None
    
    # Token 1: novel view flag, e.g. "novelTrue" or "novelFalse"
    if tokens[1].startswith("novel"):
        novel_val = tokens[1][len("novel"):]
        params["novel_view"] = True if novel_val.lower() in ("true", "1") else False
    else:
        params["novel_view"] = None
    
    # Token 2: start sampling ratio, e.g. "start0.7"
    if tokens[2].startswith("start"):
        start_val = tokens[2][len("start"):]
        try:
            params["start_sampling_ratio"] = float(start_val)
        except:
            params["start_sampling_ratio"] = start_val
    else:
        params["start_sampling_ratio"] = None
    
    # Token 3: target sampling ratio, e.g. "target0.9"
    if tokens[3].startswith("target"):
        target_val = tokens[3][len("target"):]
        try:
            params["target_sampling_ratio"] = float(target_val)
        except:
            params["target_sampling_ratio"] = target_val
    else:
        params["target_sampling_ratio"] = None
    
    # Token 4: distill_sh_lambda, e.g. "sh0.25"
    if tokens[4].startswith("sh"):
        sh_val = tokens[4][len("sh"):]
        try:
            params["distill_sh_lambda"] = float(sh_val)
        except:
            params["distill_sh_lambda"] = sh_val
    else:
        params["distill_sh_lambda"] = tokens[4]
    
    # Token 5: distill_colors_lambda, e.g. "colors0.25"
    if tokens[5].startswith("colors"):
        colors_val = tokens[5][len("colors"):]
        try:
            params["distill_colors_lambda"] = float(colors_val)
        except:
            params["distill_colors_lambda"] = colors_val
    else:
        params["distill_colors_lambda"] = tokens[5]
    
    # Token 6: distill_depth_lambda, e.g. "depth0.25"
    if tokens[6].startswith("depth"):
        depth_val = tokens[6][len("depth"):]
        try:
            params["distill_depth_lambda"] = float(depth_val)
        except:
            params["distill_depth_lambda"] = depth_val
    else:
        params["distill_depth_lambda"] = tokens[6]
    
    # Token 7: distill_xyzs_lambda, e.g. "xyzs0.25"
    if tokens[7].startswith("xyzs"):
        xyzs_val = tokens[7][len("xyzs"):]
        try:
            params["distill_xyzs_lambda"] = float(xyzs_val)
        except:
            params["distill_xyzs_lambda"] = xyzs_val
    else:
        params["distill_xyzs_lambda"] = tokens[7]
    
    # Token 8: distill_quats_lambda, e.g. "quats0.25"
    if tokens[8].startswith("quats"):
        quats_val = tokens[8][len("quats"):]
        try:
            params["distill_quats_lambda"] = float(quats_val)
        except:
            params["distill_quats_lambda"] = quats_val
    else:
        params["distill_quats_lambda"] = tokens[8]
    
    # Process remaining tokens for gradient key and multipliers.
    remaining = tokens[9:]
    gradient_tokens = []
    multiplier_tokens = []
    # Known multiplier prefixes: shmult, depth, grow2d.
    for token in remaining:
        if re.match(r'^(shmult|depth|grow2d)([-+]?[0-9]*\.?[0-9]+)$', token):
            multiplier_tokens.append(token)
        else:
            gradient_tokens.append(token)
    
    params["gradient_key"] = "_".join(gradient_tokens) if gradient_tokens else ""
    
    # Process multiplier tokens.
    for token in multiplier_tokens:
        m = re.match(r'^shmult([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            try:
                params["sh_coeffs_mult"] = float(m.group(1))
            except:
                params["sh_coeffs_mult"] = m.group(1)
            continue
        m = re.match(r'^depth([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            try:
                params["depths_mult"] = float(m.group(1))
            except:
                params["depths_mult"] = m.group(1)
            continue
        m = re.match(r'^grow2d([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            try:
                params["grow_grad2d"] = float(m.group(1))
            except:
                params["grow_grad2d"] = m.group(1)
            continue

    return params

def analyze_students_dir(students_dir, output_csv, prefix="bicycle_4_SMALL"):
    """
    Recursively scan the given students_dir for subdirectories that contain a
    stats/val_step14999.json file. For each valid JSON, read the stats and parse
    the subdirectory name (using the new training script naming convention) to extract
    training parameters. Then, a dedicated teacher reference row is added from:
    gsplat_teachers/{prefix}/stats/val_step14999.json.
    """
    results = []
    
    # Process student directories.
    for root in os.listdir(students_dir):
        root = os.path.join(students_dir, root)
        stats_dir = os.path.join(root, "stats")
        json_file = os.path.join(stats_dir, "val_step14999.json")
        print(json_file)
        if os.path.exists(json_file):
            print('found json file', json_file)
            subdir_name = os.path.basename(root)
            try:
                with open(json_file, 'r') as f:
                    stats = json.load(f)
            except Exception as e:
                print(f"Error reading {json_file}: {e}")
                continue
            
            parsed_params = parse_subdir_name(subdir_name, prefix=prefix)
            combined = {"subdirectory": subdir_name}
            combined.update(parsed_params)
            combined.update(stats)
            results.append(combined)
    
    # Add the fixed teacher reference case.
    teacher_stats_file = os.path.join('/Bean/log/gwangjin/2025/kdgs/ms/bicycle_depth_reinit_sampling_0.5/stats/val_step29999.json')
    if os.path.exists(teacher_stats_file):
        try:
            with open(teacher_stats_file, 'r') as f:
                teacher_stats = json.load(f)
            teacher_row = {
                "subdirectory": "teacher_reference",
                "distill_sh_lambda": "",
                "distill_colors_lambda": "",
                "distill_depth_lambda": "",
                "distill_xyzs_lambda": "",
                "distill_quats_lambda": "",
                "blur": "",
                "novel_view": "",
                "start_sampling_ratio": "",
                "target_sampling_ratio": "",
                "gradient_key": "teacher",
                "sh_coeffs_mult": "",
                "depths_mult": "",
                "grow_grad2d": "",
            }
            teacher_row.update(teacher_stats)
            results.append(teacher_row)
        except Exception as e:
            print(f"Error reading teacher stats file {teacher_stats_file}: {e}")
    else:
        print(f"Teacher stats file not found: {teacher_stats_file}")

    teacher_stats_file = os.path.join('/Bean/log/gwangjin/2025/kdgs/ms_d/bicycle_depth_reinit/stats/val_step29999.json')
    if os.path.exists(teacher_stats_file):
        try:
            with open(teacher_stats_file, 'r') as f:
                teacher_stats = json.load(f)
            teacher_row = {
                "subdirectory": "teacher_reference",
                "distill_sh_lambda": "",
                "distill_colors_lambda": "",
                "distill_depth_lambda": "",
                "distill_xyzs_lambda": "",
                "distill_quats_lambda": "",
                "blur": "",
                "novel_view": "",
                "start_sampling_ratio": "",
                "target_sampling_ratio": "",
                "gradient_key": "teacher",
                "sh_coeffs_mult": "",
                "depths_mult": "",
                "grow_grad2d": "",
            }
            teacher_row.update(teacher_stats)
            results.append(teacher_row)
        except Exception as e:
            print(f"Error reading teacher stats file {teacher_stats_file}: {e}")

    
    if not results:
        print("No valid JSON files found.")
        return
    
    # Sort the results by PSNR and SSIM in descending order.
    results.sort(key=lambda x: (x.get("psnr", 0), x.get("ssim", 0)), reverse=True)
    
    header = [
        "subdirectory",
        "blur",
        "novel_view",
        "start_sampling_ratio",
        "target_sampling_ratio",
        "distill_sh_lambda",
        "distill_colors_lambda",
        "distill_depth_lambda",
        "distill_xyzs_lambda",
        "distill_quats_lambda",
        "gradient_key",
        "sh_coeffs_mult",
        "depths_mult",
        "grow_grad2d",
        "psnr",
        "ssim",
        "lpips",
        "ellipse_time",
        "num_GS"
    ]
    
    with open(output_csv, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=header)
        writer.writeheader()
        for item in results:
            row = {key: item.get(key, "") for key in header}
            writer.writerow(row)
    
    print(f"Analysis complete. Data saved to {output_csv}")

def main():
    parser = argparse.ArgumentParser(
        description="Analyze student subdirectories for stats and training parameters from val_step29999.json and add a teacher reference case."
    )
    parser.add_argument('--students_dir', type=str, default='../gsplat_students_v6', 
                        help="Path to the directory containing student subdirectories")
    parser.add_argument('--output_csv', type=str, default='reduced_results.csv', 
                        help="Path to output CSV file (default: analysis_results.csv)")
    parser.add_argument('--prefix', type=str, default='bicycle_4_SMALL', 
                        help="Prefix used in subdirectory naming (default: 'bicycle_4_SMALL')")
    
    args = parser.parse_args()
    analyze_students_dir(args.students_dir, args.output_csv, prefix=args.prefix)

if __name__ == "__main__":
    main()
