import os
import json
import csv
import argparse

def parse_subdir_name(subdir_name, prefix="bicycle_4"):
    """
    Parse a subdirectory name generated by the training script.
    Expected naming patterns:
      1. When all five lambdas are used:
         bicycle_4_sh{sh}_colors{colors}_depth{depth}_xyzs{xyzs}_quats{quats}_{gradient_key}_...
      2. Only sh lambda > 0:
         bicycle_4_sh{sh}_colors{colors}_depth{depth}_xyzs{xyzs}_quats{quats}_{gradient_key}_...
      3. When both sh and depth lambdas are > 0:
         (Same as above; gradient key and multipliers appended.)
      4. Both lambdas are zero:
         bicycle_4_sh{sh}_colors{colors}_depth{depth}_xyzs{xyzs}_quats{quats}
      5. The 'means2d' case:
         bicycle_4_sh{sh}_colors{colors}_depth{depth}_xyzs{xyzs}_quats{quats}_means2d_grow2d{grow_grad2d}
    
    This function extracts:
      - distill_sh_lambda, distill_colors_lambda, distill_depth_lambda,
        distill_xyzs_lambda, distill_quats_lambda
      - gradient_key, sh_coeffs_mult, depths_mult, grow_grad2d
    """
    params = {}
    if subdir_name.startswith(prefix):
        rest = subdir_name[len(prefix):].lstrip('_')
        tokens = rest.split('_')
    else:
        tokens = subdir_name.split('_')
    
    # We expect at least 5 tokens for the five lambdas.
    if len(tokens) < 5:
        return params

    # Extract the five lambda tokens.
    if tokens[0].startswith("sh"):
        params["distill_sh_lambda"] = tokens[0][2:]
    else:
        params["distill_sh_lambda"] = tokens[0]
    if tokens[1].startswith("colors"):
        params["distill_colors_lambda"] = tokens[1][6:]
    else:
        params["distill_colors_lambda"] = tokens[1]
    if tokens[2].startswith("depth"):
        params["distill_depth_lambda"] = tokens[2][5:]
    else:
        params["distill_depth_lambda"] = tokens[2]
    if tokens[3].startswith("xyzs"):
        params["distill_xyzs_lambda"] = tokens[3][4:]
    else:
        params["distill_xyzs_lambda"] = tokens[3]
    if tokens[4].startswith("quats"):
        params["distill_quats_lambda"] = tokens[4][5:]
    else:
        params["distill_quats_lambda"] = tokens[4]
    
    # Process remaining tokens for gradient key and multipliers.
    remaining = tokens[5:]
    if not remaining:
        return params

    # Handle the means2d case separately.
    if remaining[0] == "means2d":
        params["gradient_key"] = "means2d"
        for token in remaining[1:]:
            if token.startswith("grow2d"):
                params["grow_grad2d"] = token[6:]
        return params

    # Convert lambda strings to floats to decide how many multiplier tokens are expected.
    try:
        sh_lambda = float(params["distill_sh_lambda"])
    except ValueError:
        sh_lambda = 0.0
    try:
        depth_lambda = float(params["distill_depth_lambda"])
    except ValueError:
        depth_lambda = 0.0

    total_remaining = len(remaining)
    # Case 1: Both sh and depth lambdas > 0 -> last three tokens: sh multiplier, depth multiplier, grow2d.
    if sh_lambda > 0 and depth_lambda > 0:
        if total_remaining >= 4:
            gradient_key_tokens = remaining[:total_remaining - 3]
            params["gradient_key"] = "_".join(gradient_key_tokens)
            for token in remaining[-3:]:
                if token.startswith("sh"):
                    params["sh_coeffs_mult"] = token[2:]
                elif token.startswith("depth"):
                    params["depths_mult"] = token[5:]
                elif token.startswith("grow2d"):
                    params["grow_grad2d"] = token[6:]
        else:
            params["gradient_key"] = remaining[0]
    # Case 2: Only sh lambda > 0 -> last two tokens: sh multiplier and grow2d.
    elif sh_lambda > 0:
        if total_remaining >= 3:
            gradient_key_tokens = remaining[:total_remaining - 2]
            params["gradient_key"] = "_".join(gradient_key_tokens)
            for token in remaining[-2:]:
                if token.startswith("sh"):
                    params["sh_coeffs_mult"] = token[2:]
                elif token.startswith("grow2d"):
                    params["grow_grad2d"] = token[6:]
        else:
            params["gradient_key"] = remaining[0]
    # Case 3: Only depth lambda > 0 -> last two tokens: depth multiplier and grow2d.
    elif depth_lambda > 0:
        if total_remaining >= 3:
            gradient_key_tokens = remaining[:total_remaining - 2]
            params["gradient_key"] = "_".join(gradient_key_tokens)
            for token in remaining[-2:]:
                if token.startswith("depth"):
                    params["depths_mult"] = token[5:]
                elif token.startswith("grow2d"):
                    params["grow_grad2d"] = token[6:]
        else:
            params["gradient_key"] = remaining[0]
    else:
        # Both lambdas are zero; join all remaining tokens.
        params["gradient_key"] = "_".join(remaining)
    
    return params

def analyze_students_dir(students_dir, output_csv, prefix="bicycle_4"):
    """
    Recursively scan the given students_dir for subdirectories that contain a
    stats/val_step9999.json file. For each valid JSON, read the stats and parse
    the subdirectory name (using the training script naming convention) to extract
    training parameters. Then, a dedicated teacher reference row is added from:
    gsplat_teachers/bicycle_4/stats/val_step29999.json.
    The combined data is then saved into a CSV file.
    """
    results = []
    
    # Process student directories.
    for root, dirs, files in os.walk(students_dir):
        stats_dir = os.path.join(root, "stats")
        json_file = os.path.join(stats_dir, "val_step9999.json")
        if os.path.exists(json_file):
            subdir_name = os.path.basename(root)
            try:
                with open(json_file, 'r') as f:
                    stats = json.load(f)
            except Exception as e:
                print(f"Error reading {json_file}: {e}")
                continue
            
            parsed_params = parse_subdir_name(subdir_name, prefix=prefix)
            combined = {"subdirectory": subdir_name}
            combined.update(parsed_params)
            combined.update(stats)
            results.append(combined)
    
    # Add the fixed teacher reference case.
    teacher_stats_file = os.path.join("gsplat_teachers", "bicycle_4", "stats", "val_step29999.json")
    if os.path.exists(teacher_stats_file):
        try:
            with open(teacher_stats_file, 'r') as f:
                teacher_stats = json.load(f)
            teacher_row = {
                "subdirectory": "teacher_reference",
                "distill_sh_lambda": "",
                "distill_colors_lambda": "",
                "distill_depth_lambda": "",
                "distill_xyzs_lambda": "",
                "distill_quats_lambda": "",
                "gradient_key": "teacher",
                "sh_coeffs_mult": "",
                "depths_mult": "",
                "grow_grad2d": "",
            }
            teacher_row.update(teacher_stats)
            results.append(teacher_row)
        except Exception as e:
            print(f"Error reading teacher stats file {teacher_stats_file}: {e}")
    else:
        print(f"Teacher stats file not found: {teacher_stats_file}")
    
    if not results:
        print("No valid JSON files found.")
        return

    # Sort the results by psnr and ssim in descending order.
    results.sort(key=lambda x: (x.get("psnr", 0), x.get("ssim", 0)), reverse=True)
    
    header = [
        "subdirectory",
        "distill_sh_lambda",
        "distill_colors_lambda",
        "distill_depth_lambda",
        "distill_xyzs_lambda",
        "distill_quats_lambda",
        "gradient_key",
        "sh_coeffs_mult",
        "depths_mult",
        "grow_grad2d",
        "psnr",
        "ssim",
        "lpips",
        "ellipse_time",
        "num_GS"
    ]
    
    with open(output_csv, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=header)
        writer.writeheader()
        for item in results:
            row = {key: item.get(key, "") for key in header}
            writer.writerow(row)
    
    print(f"Analysis complete. Data saved to {output_csv}")

def main():
    parser = argparse.ArgumentParser(
        description="Analyze student subdirectories for stats and training parameters from val_step9999.json and add a teacher reference case."
    )
    parser.add_argument('--students_dir', type=str, default='../gsplat_students_v6', 
                        help="Path to the directory containing student subdirectories")
    parser.add_argument('--output_csv', type=str, default='analysis_results.csv', 
                        help="Path to output CSV file (default: analysis_results.csv)")
    parser.add_argument('--prefix', type=str, default='bicycle_4', 
                        help="Prefix used in subdirectory naming (default: 'bicycle_4')")
    
    args = parser.parse_args()
    analyze_students_dir(args.students_dir, args.output_csv, prefix=args.prefix)

if __name__ == "__main__":
    main()
