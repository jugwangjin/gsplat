import os
import json
import csv
import argparse
import re
from collections import defaultdict

def parse_subdir_name(subdir_name, prefix="bicycle_4_SMALL"):
    """
    Parse a subdirectory name generated by the new training script.
    
    Expected naming pattern:
      For directories with densification:
      prefix_blur{blur}_novel{novel_view}_densify{densification}_start{start_sampling_ratio}_target{target_sampling_ratio}
      _sh{distill_sh_lambda}_colors{distill_colors_lambda}_depth{distill_depth_lambda}_xyzs{distill_xyzs_lambda}_quats{distill_quats_lambda}
      _{gradient_key}[_shmult{sh_coeffs_mult}][_depth{depths_mult}]_grow2d{grow_grad2d}
      
      For backward compatibility (without densification), the tokens after novel_view remain the same.
      
    This function extracts:
      - blur (bool)
      - novel_view (bool)
      - densification (bool, if available)
      - start_sampling_ratio (float)
      - target_sampling_ratio (float)
      - distill_sh_lambda, distill_colors_lambda, distill_depth_lambda,
        distill_xyzs_lambda, distill_quats_lambda (floats)
      - gradient_key (str)
      - sh_coeffs_mult (float, optional)
      - depths_mult (float, optional)
      - grow_grad2d (float)
    """
    params = {}
    prefix2 = "bicycle_4_FULL"
    # Remove the prefix if present.
    if subdir_name.startswith(prefix):
        rest = subdir_name[len(prefix):].lstrip('_')
    elif subdir_name.startswith(prefix2):
        rest = subdir_name[len(prefix2):].lstrip('_')
    else:
        rest = subdir_name
    tokens = rest.split('_')
    
    # Determine if densification token is present.
    offset = 0
    if len(tokens) >= 3 and tokens[2].startswith("densify"):
        densify_val = tokens[2][len("densify"):]
        params["densification"] = True if densify_val.lower() in ("true", "1") else False
        offset = 1
    else:
        params["densification"] = None  # not specified

    # We expect at least 9 tokens for the mandatory parameters (adjusted for offset).
    if len(tokens) < 9 + offset:
        return params
    
    # Token 0: blur flag, e.g. "blurTrue" or "blurFalse"
    if tokens[0].startswith("blur"):
        blur_val = tokens[0][len("blur"):]
        params["blur"] = True if blur_val.lower() in ("true", "1") else False
    else:
        params["blur"] = None
    
    # Token 1: novel view flag, e.g. "novelTrue" or "novelFalse"
    if tokens[1].startswith("novel"):
        novel_val = tokens[1][len("novel"):]
        params["novel_view"] = True if novel_val.lower() in ("true", "1") else False
    else:
        params["novel_view"] = None
    
    start_idx = 2 + offset

    # Token for start sampling ratio, e.g. "start0.7"
    if tokens[start_idx].startswith("start"):
        start_val = tokens[start_idx][len("start"):]
        try:
            params["start_sampling_ratio"] = float(start_val)
        except:
            params["start_sampling_ratio"] = start_val
    else:
        params["start_sampling_ratio"] = None
    
    # Next token: target sampling ratio, e.g. "target0.9"
    if tokens[start_idx+1].startswith("target"):
        target_val = tokens[start_idx+1][len("target"):]
        try:
            params["target_sampling_ratio"] = float(target_val)
        except:
            params["target_sampling_ratio"] = target_val
    else:
        params["target_sampling_ratio"] = None
    
    # Next tokens: lambdas.
    if tokens[start_idx+2].startswith("sh"):
        sh_val = tokens[start_idx+2][len("sh"):]
        try:
            params["distill_sh_lambda"] = float(sh_val)
        except:
            params["distill_sh_lambda"] = sh_val
    else:
        params["distill_sh_lambda"] = tokens[start_idx+2]
    
    if tokens[start_idx+3].startswith("colors"):
        colors_val = tokens[start_idx+3][len("colors"):]
        try:
            params["distill_colors_lambda"] = float(colors_val)
        except:
            params["distill_colors_lambda"] = colors_val
    else:
        params["distill_colors_lambda"] = tokens[start_idx+3]
    
    if tokens[start_idx+4].startswith("depth"):
        depth_val = tokens[start_idx+4][len("depth"):]
        try:
            params["distill_depth_lambda"] = float(depth_val)
        except:
            params["distill_depth_lambda"] = depth_val
    else:
        params["distill_depth_lambda"] = tokens[start_idx+4]
    
    if tokens[start_idx+5].startswith("xyzs"):
        xyzs_val = tokens[start_idx+5][len("xyzs"):]
        try:
            params["distill_xyzs_lambda"] = float(xyzs_val)
        except:
            params["distill_xyzs_lambda"] = xyzs_val
    else:
        params["distill_xyzs_lambda"] = tokens[start_idx+5]
    
    if tokens[start_idx+6].startswith("quats"):
        quats_val = tokens[start_idx+6][len("quats"):]
        try:
            params["distill_quats_lambda"] = float(quats_val)
        except:
            params["distill_quats_lambda"] = quats_val
    else:
        params["distill_quats_lambda"] = tokens[start_idx+6]
    
    # Process remaining tokens for gradient key and multipliers.
    remaining = tokens[start_idx+7:]
    gradient_tokens = []
    multiplier_tokens = []
    # Known multiplier prefixes: shmult, depth, grow2d.
    for token in remaining:
        if re.match(r'^(shmult|depth|grow2d)([-+]?[0-9]*\.?[0-9]+)$', token):
            multiplier_tokens.append(token)
        else:
            gradient_tokens.append(token)
    
    params["gradient_key"] = "_".join(gradient_tokens) if gradient_tokens else ""
    
    # Process multiplier tokens.
    for token in multiplier_tokens:
        m = re.match(r'^shmult([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            try:
                params["sh_coeffs_mult"] = float(m.group(1))
            except:
                params["sh_coeffs_mult"] = m.group(1)
            continue
        m = re.match(r'^depth([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            try:
                params["depths_mult"] = float(m.group(1))
            except:
                params["depths_mult"] = m.group(1)
            continue
        m = re.match(r'^grow2d([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            try:
                params["grow_grad2d"] = float(m.group(1))
            except:
                params["grow_grad2d"] = m.group(1)
            continue

    return params

def group_and_write_csv(results, output_csv_grouped, known_prefixes):
    """
    Create a new CSV file grouping student results by prefix and target sampling ratio.
    The CSV has the following columns:
      1. prefix (extracted from the subdirectory name)
      2. target_sampling_ratio
      3. model_name (constructed using:
            - If novel_view is True: add 'NV'
            - If densification is True: add 'D'
            - If any of the distill_*_lambda values (if convertible to float) is > 0: add 'KT'
         )
      4. psnr
      5. ssim
      6. num_GS
    A blank row is inserted after each group.
    
    For methods with the same model name, only the row with the highest PSNR is kept.
    """
    def extract_prefix(subdir_name):
        for p in known_prefixes:
            if subdir_name.startswith(p):
                return p
        return subdir_name.split('_')[0]

    def to_float(val):
        try:
            return float(val)
        except:
            return 0.0

    def compute_model_name(row):
        model_name = ""
        if row.get("novel_view") is True or str(row.get("novel_view")).lower() == "true":
            model_name += "+NV"
        if row.get("densification") is True or str(row.get("densification")).lower() == "true":
            model_name += "+D"
        lambdas = [
            to_float(row.get("distill_sh_lambda", 0)),
            to_float(row.get("distill_colors_lambda", 0)),
            to_float(row.get("distill_depth_lambda", 0)),
            to_float(row.get("distill_xyzs_lambda", 0)),
            to_float(row.get("distill_quats_lambda", 0))
        ]
        if any(l > 0 for l in lambdas):
            model_name += "+KT"
        return model_name

    groups = defaultdict(list)
    for row in results:
        subdir = row.get("subdirectory", "")
        prefix_val = extract_prefix(subdir)
        target = row.get("target_sampling_ratio", "")
        groups[(prefix_val, target)].append(row)
    
    header = ["prefix", "target_sampling_ratio", "model_name", "psnr", "ssim", "num_GS"]
    with open(output_csv_grouped, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=header)
        writer.writeheader()
        
        for group_key in sorted(groups.keys(), key=lambda k: (k[0], k[1])):
            prefix_val, target = group_key
            group_rows = groups[group_key]
            model_reps = {}
            for row in group_rows:
                model_name = compute_model_name(row)
                psnr_val = to_float(row.get("psnr", 0))
                if model_name not in model_reps or psnr_val > to_float(model_reps[model_name].get("psnr", 0)):
                    model_reps[model_name] = row
            rep_rows = list(model_reps.values())
            rep_rows.sort(key=lambda r: compute_model_name(r))
            for row in rep_rows:
                writer.writerow({
                    "prefix": prefix_val,
                    "target_sampling_ratio": target,
                    "model_name": compute_model_name(row),
                    "psnr": row.get("psnr", ""),
                    "ssim": row.get("ssim", ""),
                    "num_GS": row.get("num_GS", "")
                })
            writer.writerow({col: "" for col in header})

def analyze_dir(base_dir, mode, json_filename, prefix="bicycle_4_SMALL"):
    """
    Scan a given directory for subdirectories that contain a valid stats JSON file.
    The mode parameter determines whether the directory is for student or teacher runs.
    
    For student mode, the subdirectory name is parsed using parse_subdir_name.
    For teacher mode, a simple teacher type extraction is done based on the subdir name.
    """
    results = []
    for subdir in os.listdir(base_dir):
        subdir_path = os.path.join(base_dir, subdir)
        if not os.path.isdir(subdir_path):
            continue
        stats_dir = os.path.join(subdir_path, "stats")
        stats_file = os.path.join(stats_dir, json_filename)
        if os.path.exists(stats_file):
            print(f"Found {mode} stats: {stats_file}")
            try:
                with open(stats_file, 'r') as f:
                    stats = json.load(f)
            except Exception as e:
                print(f"Error reading {stats_file}: {e}")
                continue
            
            entry = {"subdirectory": subdir, "mode": mode}
            if mode == "student":
                parsed_params = parse_subdir_name(subdir, prefix=prefix)
                entry.update(parsed_params)
            elif mode == "teacher":
                # For teacher directories, extract teacher type from the folder name.
                teacher_type = ""
                if "dense" in subdir:
                    teacher_type = "full"
                elif "small" in subdir:
                    teacher_type = "small"
                entry["teacher_type"] = teacher_type
            entry.update(stats)
            results.append(entry)
    return results

def analyze_training_results(students_dir, teachers_dir, output_csv, prefix):
    results = []
    if students_dir and os.path.exists(students_dir):
        print(f"Analyzing student results in {students_dir}")
        student_results = analyze_dir(students_dir, "student", "val_step14999.json", prefix=prefix)
        results.extend(student_results)
    else:
        print("No valid students directory provided or directory does not exist.")
    
    if teachers_dir and os.path.exists(teachers_dir):
        print(f"Analyzing teacher results in {teachers_dir}")
        teacher_results = analyze_dir(teachers_dir, "teacher", "val_step29999.json", prefix=prefix)
        results.extend(teacher_results)
    else:
        print("No valid teachers directory provided or directory does not exist.")
    
    if not results:
        print("No valid JSON files found.")
        return

    def safe_float(val):
        try:
            return float(val)
        except:
            return 0.0

    results.sort(key=lambda x: (
        x.get("mode", ""),
        safe_float(x.get("target_sampling_ratio", 0)),
        safe_float(x.get("psnr", 0)),
        safe_float(x.get("ssim", 0))
    ), reverse=True)

    header = [
        "subdirectory",
        "mode",
        "teacher_type",
        "blur",
        "novel_view",
        "densification",
        "start_sampling_ratio",
        "target_sampling_ratio",
        "distill_sh_lambda",
        "distill_colors_lambda",
        "distill_depth_lambda",
        "distill_xyzs_lambda",
        "distill_quats_lambda",
        "gradient_key",
        "sh_coeffs_mult",
        "depths_mult",
        "grow_grad2d",
        "psnr",
        "ssim",
        "lpips",
        "ellipse_time",
        "num_GS"
    ]
    
    with open(output_csv, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=header)
        writer.writeheader()
        for item in results:
            row = {key: item.get(key, "") for key in header}
            writer.writerow(row)
    
    print(f"Analysis complete. Data saved to {output_csv}")
    
    # For student results, generate a grouped CSV.
    student_results = [r for r in results if r.get("mode") == "student"]
    if student_results:
        grouped_csv = "grouped_student_results.csv"
        group_and_write_csv(student_results, grouped_csv, known_prefixes=[prefix, "bicycle_4_FULL"])
        print(f"Grouped student CSV saved to {grouped_csv}")

def main():
    parser = argparse.ArgumentParser(
        description="Analyze training results from student and teacher subdirectories."
    )
    parser.add_argument('--students_dir', type=str, default='../gsplat_students_v6', 
                        help="Path to the directory containing student subdirectories")
    parser.add_argument('--teachers_dir', type=str, default='/Bean/log/gwangjin/2025/kdgs/teachers', 
                        help="Path to the directory containing teacher subdirectories")
    parser.add_argument('--output_csv', type=str, default='datasets_combined_results.csv', 
                        help="Path to output CSV file (default: combined_results.csv)")
    parser.add_argument('--prefix', type=str, default='bicycle_4_SMALL', 
                        help="Prefix used in student subdirectory naming (default: 'bicycle_4_SMALL')")
    
    args = parser.parse_args()
    analyze_training_results(args.students_dir, args.teachers_dir, args.output_csv, args.prefix)

if __name__ == "__main__":
    main()
