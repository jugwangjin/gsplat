import os
import json
import csv
import argparse
import re

def parse_subdir_name(subdir_name, prefix="bicycle_4"):
    """
    Parse a subdirectory name generated by the training script.

    Expected naming patterns include tokens for:
      - The five distillation lambdas: sh, colors, depth, xyzs, quats.
      - A gradient key that may contain underscores (e.g. "rendered_sh_coeffs" or "means2d").
      - Multiplier tokens prefixed with: shmult, depth, and grow2d.
      - At the end, two tokens of the form blur{True/False} and novelview{True/False},
        which may sometimes include an extra underscore (e.g. blur_True).

    This function extracts:
      - distill_sh_lambda, distill_colors_lambda, distill_depth_lambda,
        distill_xyzs_lambda, distill_quats_lambda
      - gradient_key, sh_coeffs_mult, depths_mult, grow_grad2d
      - blur and novel_view flags.
    """
    params = {}

    # Remove the prefix if present.
    if subdir_name.startswith(prefix):
        rest = subdir_name[len(prefix):].lstrip('_')
    else:
        rest = subdir_name
    tokens = rest.split('_')

    # Extract blur and novelview tokens if they exist at the end.
    # We allow an optional extra underscore after the keyword.
    if len(tokens) >= 2 and re.match(r'^blur[_]?[A-Za-z0-9]+', tokens[-2], re.IGNORECASE) and re.match(r'^novelview[_]?[A-Za-z0-9]*', tokens[-1], re.IGNORECASE):
        blur_token = tokens.pop(-2)
        novelview_token = tokens.pop(-1)
        blur_val = blur_token[len("blur"):].lstrip("_")
        novelview_val = novelview_token[len("novelview"):].lstrip("_")
        params["blur"] = True if blur_val.lower() in ("true", "1") else False
        params["novel_view"] = True if novelview_val.lower() in ("true", "1") else False
    else:
        params["blur"] = None
        params["novel_view"] = "novelview" in subdir_name.lower()

    # Expect the first five tokens for the lambdas.
    if len(tokens) < 5:
        return params

    # Extract the five lambda values.
    lambda_keys = [
        ("distill_sh_lambda", "sh"),
        ("distill_colors_lambda", "colors"),
        ("distill_depth_lambda", "depth"),
        ("distill_xyzs_lambda", "xyzs"),
        ("distill_quats_lambda", "quats")
    ]
    for i, (param_name, key) in enumerate(lambda_keys):
        token = tokens[i]
        if token.startswith(key):
            params[param_name] = token[len(key):]
        else:
            params[param_name] = token

    # Process the remaining tokens for gradient key and multipliers.
    remaining = tokens[5:]
    if not remaining:
        return params

    # Special case: if the first remaining token is exactly "means2d"
    if remaining[0] == "means2d":
        params["gradient_key"] = "means2d"
        for token in remaining[1:]:
            m = re.match(r'^grow2d([-+]?[0-9]*\.?[0-9]+)$', token)
            if m:
                params["grow_grad2d"] = m.group(1)
        return params

    # Otherwise, determine which tokens belong to the gradient key.
    # The idea is that multiplier tokens start with shmult, depth, or grow2d.
    gradient_tokens = []
    for token in remaining:
        if re.match(r'^shmult([-+]?[0-9]*\.?[0-9]+)$', token) or \
           re.match(r'^depth([-+]?[0-9]*\.?[0-9]+)$', token) or \
           re.match(r'^grow2d([-+]?[0-9]*\.?[0-9]+)$', token):
            break
        else:
            gradient_tokens.append(token)
    params["gradient_key"] = "_".join(gradient_tokens) if gradient_tokens else ""

    # Process the multiplier tokens from the remaining list.
    multiplier_tokens = remaining[len(gradient_tokens):]
    for token in multiplier_tokens:
        m = re.match(r'^shmult([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            params["sh_coeffs_mult"] = m.group(1)
            continue
        m = re.match(r'^depth([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            params["depths_mult"] = m.group(1)
            continue
        m = re.match(r'^grow2d([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            params["grow_grad2d"] = m.group(1)
            continue

    return params

def analyze_students_dir(students_dir, output_csv, prefix="bicycle_4"):
    """
    Recursively scan the given students_dir for subdirectories that contain a
    stats/val_step29999.json file. For each valid JSON, read the stats and parse
    the subdirectory name (using the training script naming convention) to extract
    training parameters. Then, a dedicated teacher reference row is added from:
    gsplat_teachers/{prefix}/stats/val_step29999.json.

    A new column "novel_view" (and "blur") is added based on the parsed values.
    """
    results = []
    
    # Process student directories.
    for root, dirs, files in os.walk(students_dir):
        stats_dir = os.path.join(root, "stats")
        json_file = os.path.join(stats_dir, "val_step29999.json")
        if os.path.exists(json_file):
            subdir_name = os.path.basename(root)
            try:
                with open(json_file, 'r') as f:
                    stats = json.load(f)
            except Exception as e:
                print(f"Error reading {json_file}: {e}")
                continue
            
            parsed_params = parse_subdir_name(subdir_name, prefix=prefix)
            combined = {"subdirectory": subdir_name}
            # Ensure novel_view and blur flags are set.
            combined["novel_view"] = parsed_params.get("novel_view", "novelview" in subdir_name.lower())
            combined["blur"] = parsed_params.get("blur", None)
            combined.update(parsed_params)
            combined.update(stats)
            results.append(combined)
    
    # Add the fixed teacher reference case.
    teacher_stats_file = os.path.join("gsplat_teachers", prefix, "stats", "val_step29999.json")
    if os.path.exists(teacher_stats_file):
        try:
            with open(teacher_stats_file, 'r') as f:
                teacher_stats = json.load(f)
            teacher_row = {
                "subdirectory": "teacher_reference",
                "distill_sh_lambda": "",
                "distill_colors_lambda": "",
                "distill_depth_lambda": "",
                "distill_xyzs_lambda": "",
                "distill_quats_lambda": "",
                "blur": "",
                "novel_view": False,
                "gradient_key": "teacher",
                "sh_coeffs_mult": "",
                "depths_mult": "",
                "grow_grad2d": "",
            }
            teacher_row.update(teacher_stats)
            results.append(teacher_row)
        except Exception as e:
            print(f"Error reading teacher stats file {teacher_stats_file}: {e}")
    else:
        print(f"Teacher stats file not found: {teacher_stats_file}")
    
    if not results:
        print("No valid JSON files found.")
        return

    # Sort the results by PSNR and SSIM in descending order.
    results.sort(key=lambda x: (x.get("psnr", 0), x.get("ssim", 0)), reverse=True)
    
    header = [
        "subdirectory",
        "distill_sh_lambda",
        "distill_colors_lambda",
        "distill_depth_lambda",
        "distill_xyzs_lambda",
        "distill_quats_lambda",
        "blur",
        "novel_view",
        "gradient_key",
        "sh_coeffs_mult",
        "depths_mult",
        "grow_grad2d",
        "psnr",
        "ssim",
        "lpips",
        "ellipse_time",
        "num_GS"
    ]
    
    with open(output_csv, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=header)
        writer.writeheader()
        for item in results:
            row = {key: item.get(key, "") for key in header}
            writer.writerow(row)
    
    print(f"Analysis complete. Data saved to {output_csv}")

def main():
    parser = argparse.ArgumentParser(
        description="Analyze student subdirectories for stats and training parameters from val_step29999.json and add a teacher reference case."
    )
    parser.add_argument('--students_dir', type=str, default='../gsplat_students_v6', 
                        help="Path to the directory containing student subdirectories")
    parser.add_argument('--output_csv', type=str, default='analysis_results.csv', 
                        help="Path to output CSV file (default: analysis_results.csv)")
    parser.add_argument('--prefix', type=str, default='bicycle_4', 
                        help="Prefix used in subdirectory naming (default: 'bicycle_4')")
    
    args = parser.parse_args()
    analyze_students_dir(args.students_dir, args.output_csv, prefix=args.prefix)

if __name__ == "__main__":
    main()
