import os
import json
import csv
import argparse
import re
from collections import defaultdict

def parse_subdir_name(subdir_name, prefix="bicycle_4_SMALL"):
    """
    Parse a subdirectory name generated by the new training script.
    
    Expected naming pattern:
      For directories with densification:
      prefix_blur{blur}_novel{novel_view}_densify{densification}_start{start_sampling_ratio}_target{target_sampling_ratio}
      _sh{distill_sh_lambda}_colors{distill_colors_lambda}_depth{distill_depth_lambda}_xyzs{distill_xyzs_lambda}_quats{distill_quats_lambda}
      _{gradient_key}[_shmult{sh_coeffs_mult}][_depth{depths_mult}]_grow2d{grow_grad2d}
      
      For backward compatibility (without densification), the tokens after novel_view remain the same.
      
    This function extracts:
      - blur (bool)
      - novel_view (bool)
      - densification (bool, if available)
      - start_sampling_ratio (float)
      - target_sampling_ratio (float)
      - distill_sh_lambda, distill_colors_lambda, distill_depth_lambda,
        distill_xyzs_lambda, distill_quats_lambda (floats)
      - gradient_key (str)
      - sh_coeffs_mult (float, optional)
      - depths_mult (float, optional)
      - grow_grad2d (float)
    """
    params = {}
    prefix2 = "bicycle_4_FULL"
    # Remove the prefix if present.
    if subdir_name.startswith(prefix):
        rest = subdir_name[len(prefix):].lstrip('_')
    elif subdir_name.startswith(prefix2):
        rest = subdir_name[len(prefix2):].lstrip('_')
    else:
        rest = subdir_name
    tokens = rest.split('_')
    
    # Determine if densification token is present.
    # Expected token order with densification:
    # 0: blur, 1: novel_view, 2: densify, 3: start, 4: target, then lambdas...
    offset = 0
    if len(tokens) >= 3 and tokens[2].startswith("densify"):
        densify_val = tokens[2][len("densify"):]
        params["densification"] = True if densify_val.lower() in ("true", "1") else False
        offset = 1
    else:
        params["densification"] = None  # not specified

    # We expect at least 9 tokens for the mandatory parameters (adjusted for offset).
    if len(tokens) < 9 + offset:
        return params
    
    # Token 0: blur flag, e.g. "blurTrue" or "blurFalse"
    if tokens[0].startswith("blur"):
        blur_val = tokens[0][len("blur"):]
        params["blur"] = True if blur_val.lower() in ("true", "1") else False
    else:
        params["blur"] = None
    
    # Token 1: novel view flag, e.g. "novelTrue" or "novelFalse"
    if tokens[1].startswith("novel"):
        novel_val = tokens[1][len("novel"):]
        params["novel_view"] = True if novel_val.lower() in ("true", "1") else False
    else:
        params["novel_view"] = None
    
    start_idx = 2 + offset

    # Token for start sampling ratio, e.g. "start0.7"
    if tokens[start_idx].startswith("start"):
        start_val = tokens[start_idx][len("start"):]
        try:
            params["start_sampling_ratio"] = float(start_val)
        except:
            params["start_sampling_ratio"] = start_val
    else:
        params["start_sampling_ratio"] = None
    
    # Next token: target sampling ratio, e.g. "target0.9"
    if tokens[start_idx+1].startswith("target"):
        target_val = tokens[start_idx+1][len("target"):]
        try:
            params["target_sampling_ratio"] = float(target_val)
        except:
            params["target_sampling_ratio"] = target_val
    else:
        params["target_sampling_ratio"] = None
    
    # Next tokens: lambdas.
    # Token for distill_sh_lambda, e.g. "sh0.25"
    if tokens[start_idx+2].startswith("sh"):
        sh_val = tokens[start_idx+2][len("sh"):]
        try:
            params["distill_sh_lambda"] = float(sh_val)
        except:
            params["distill_sh_lambda"] = sh_val
    else:
        params["distill_sh_lambda"] = tokens[start_idx+2]
    
    # Token for distill_colors_lambda, e.g. "colors0.25"
    if tokens[start_idx+3].startswith("colors"):
        colors_val = tokens[start_idx+3][len("colors"):]
        try:
            params["distill_colors_lambda"] = float(colors_val)
        except:
            params["distill_colors_lambda"] = colors_val
    else:
        params["distill_colors_lambda"] = tokens[start_idx+3]
    
    # Token for distill_depth_lambda, e.g. "depth0.25"
    if tokens[start_idx+4].startswith("depth"):
        depth_val = tokens[start_idx+4][len("depth"):]
        try:
            params["distill_depth_lambda"] = float(depth_val)
        except:
            params["distill_depth_lambda"] = depth_val
    else:
        params["distill_depth_lambda"] = tokens[start_idx+4]
    
    # Token for distill_xyzs_lambda, e.g. "xyzs0.25"
    if tokens[start_idx+5].startswith("xyzs"):
        xyzs_val = tokens[start_idx+5][len("xyzs"):]
        try:
            params["distill_xyzs_lambda"] = float(xyzs_val)
        except:
            params["distill_xyzs_lambda"] = xyzs_val
    else:
        params["distill_xyzs_lambda"] = tokens[start_idx+5]
    
    # Token for distill_quats_lambda, e.g. "quats0.25"
    if tokens[start_idx+6].startswith("quats"):
        quats_val = tokens[start_idx+6][len("quats"):]
        try:
            params["distill_quats_lambda"] = float(quats_val)
        except:
            params["distill_quats_lambda"] = quats_val
    else:
        params["distill_quats_lambda"] = tokens[start_idx+6]
    
    # Process remaining tokens for gradient key and multipliers.
    remaining = tokens[start_idx+7:]
    gradient_tokens = []
    multiplier_tokens = []
    # Known multiplier prefixes: shmult, depth, grow2d.
    for token in remaining:
        if re.match(r'^(shmult|depth|grow2d)([-+]?[0-9]*\.?[0-9]+)$', token):
            multiplier_tokens.append(token)
        else:
            gradient_tokens.append(token)
    
    params["gradient_key"] = "_".join(gradient_tokens) if gradient_tokens else ""
    
    # Process multiplier tokens.
    for token in multiplier_tokens:
        m = re.match(r'^shmult([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            try:
                params["sh_coeffs_mult"] = float(m.group(1))
            except:
                params["sh_coeffs_mult"] = m.group(1)
            continue
        m = re.match(r'^depth([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            try:
                params["depths_mult"] = float(m.group(1))
            except:
                params["depths_mult"] = m.group(1)
            continue
        m = re.match(r'^grow2d([-+]?[0-9]*\.?[0-9]+)$', token)
        if m:
            try:
                params["grow_grad2d"] = float(m.group(1))
            except:
                params["grow_grad2d"] = m.group(1)
            continue

    return params

def group_and_write_csv(results, output_csv_grouped, known_prefixes):
    """
    Create a new CSV file grouping students by prefix and target sampling ratio.
    The CSV has the following columns:
      1. prefix (extracted from the subdirectory name)
      2. target_sampling_ratio
      3. model_name (constructed using:
            - If novel_view is True: add 'NV'
            - If densification is True: add 'D'
            - If any of the distill_*_lambda values (if convertible to float) is > 0: add 'KT'
         )
      4. psnr
      5. ssim
      6. num_GS
    A blank row is inserted after each group.
    
    For methods with the same model name, only the row with the highest PSNR is kept.
    """
    def extract_prefix(subdir_name):
        for p in known_prefixes:
            if subdir_name.startswith(p):
                return p
        return subdir_name.split('_')[0]

    def to_float(val):
        try:
            return float(val)
        except:
            return 0.0

    def compute_model_name(row):
        model_name = ""
        if row.get("novel_view") is True or str(row.get("novel_view")).lower() == "true":
            model_name += "+NV"
        if row.get("densification") is True or str(row.get("densification")).lower() == "true":
            model_name += "+D"
        lambdas = [
            to_float(row.get("distill_sh_lambda", 0)),
            to_float(row.get("distill_colors_lambda", 0)),
            to_float(row.get("distill_depth_lambda", 0)),
            to_float(row.get("distill_xyzs_lambda", 0)),
            to_float(row.get("distill_quats_lambda", 0))
        ]
        if any(l > 0 for l in lambdas):
            model_name += "+KT"
        return model_name

    groups = defaultdict(list)
    for row in results:
        subdir = row.get("subdirectory", "")
        prefix_val = extract_prefix(subdir)
        target = row.get("target_sampling_ratio", "")
        groups[(prefix_val, target)].append(row)
    
    header = ["prefix", "target_sampling_ratio", "model_name", "psnr", "ssim", "num_GS"]
    with open(output_csv_grouped, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=header)
        writer.writeheader()
        
        # Process each group.
        for group_key in sorted(groups.keys(), key=lambda k: (k[0], k[1])):
            prefix_val, target = group_key
            group_rows = groups[group_key]
            # Group by model name and select representative row (highest PSNR) for each.
            model_reps = {}
            for row in group_rows:
                model_name = compute_model_name(row)
                psnr_val = to_float(row.get("psnr", 0))
                if model_name not in model_reps or psnr_val > to_float(model_reps[model_name].get("psnr", 0)):
                    model_reps[model_name] = row
            # Get representative rows and sort them by model name.
            rep_rows = list(model_reps.values())
            rep_rows.sort(key=lambda r: compute_model_name(r))
            for row in rep_rows:
                writer.writerow({
                    "prefix": prefix_val,
                    "target_sampling_ratio": target,
                    "model_name": compute_model_name(row),
                    "psnr": row.get("psnr", ""),
                    "ssim": row.get("ssim", ""),
                    "num_GS": row.get("num_GS", "")
                })
            # Insert a blank row to separate groups.
            writer.writerow({col: "" for col in header})

def analyze_students_dir(students_dir, output_csv, prefix="bicycle_4_SMALL"):
    """
    Recursively scan the given students_dir for subdirectories that contain a
    stats/val_step14999.json file. For each valid JSON, read the stats and parse
    the subdirectory name (using the new training script naming convention) to extract
    training parameters. Then, fixed teacher reference cases are added from teacher stats files.
    """
    results = []
    
    # Process student directories.
    for root in os.listdir(students_dir):
        root = os.path.join(students_dir, root)
        stats_dir = os.path.join(root, "stats")
        json_file = os.path.join(stats_dir, "val_step14999.json")
        print(f"Checking: {json_file}")
        if os.path.exists(json_file):
            print(f"Found JSON file: {json_file}")
            subdir_name = os.path.basename(root)
            try:
                with open(json_file, 'r') as f:
                    stats = json.load(f)
            except Exception as e:
                print(f"Error reading {json_file}: {e}")
                continue
            
            parsed_params = parse_subdir_name(subdir_name, prefix=prefix)
            combined = {"subdirectory": subdir_name}
            combined.update(parsed_params)
            combined.update(stats)
            results.append(combined)
    
    # Add the fixed teacher reference cases.
    teacher_stats_files = [
        os.path.join('/Bean/log/gwangjin/2025/kdgs/ms/bicycle_depth_reinit_sampling_0.5/stats/val_step29999.json'),
        os.path.join('/Bean/log/gwangjin/2025/kdgs/ms_d/bicycle_depth_reinit/stats/val_step29999.json')
    ]
    for teacher_stats_file in teacher_stats_files:
        if os.path.exists(teacher_stats_file):
            try:
                with open(teacher_stats_file, 'r') as f:
                    teacher_stats = json.load(f)
                teacher_row = {
                    "subdirectory": "teacher_reference",
                    "distill_sh_lambda": "",
                    "distill_colors_lambda": "",
                    "distill_depth_lambda": "",
                    "distill_xyzs_lambda": "",
                    "distill_quats_lambda": "",
                    "blur": "",
                    "novel_view": "",
                    "start_sampling_ratio": "",
                    "target_sampling_ratio": "",
                    "densification": "",
                    "gradient_key": "teacher",
                    "sh_coeffs_mult": "",
                    "depths_mult": "",
                    "grow_grad2d": "",
                }
                teacher_row.update(teacher_stats)
                results.append(teacher_row)
            except Exception as e:
                print(f"Error reading teacher stats file {teacher_stats_file}: {e}")
        else:
            print(f"Teacher stats file not found: {teacher_stats_file}")
    
    if not results:
        print("No valid JSON files found.")
        return
    def safe_float(val):
        try:
            return float(val)
        except:
            return 0.0

    # Replace the old sort line with this:
    results.sort(key=lambda x: (
        bool(x.get("novel_view", False)),                                  # True first
        bool(x.get("densification", False)) if x.get("densification") is not None else False,  # True first
        safe_float(x.get("target_sampling_ratio", 0)),                      # Higher target_sampling_ratio first
        safe_float(x.get("psnr", 0)),                                         # Higher PSNR first
        safe_float(x.get("ssim", 0))                                          # Higher SSIM first
    ), reverse=True)
    
    header = [
        "subdirectory",
        "blur",
        "novel_view",
        "densification",
        "start_sampling_ratio",
        "target_sampling_ratio",
        "distill_sh_lambda",
        "distill_colors_lambda",
        "distill_depth_lambda",
        "distill_xyzs_lambda",
        "distill_quats_lambda",
        "gradient_key",
        "sh_coeffs_mult",
        "depths_mult",
        "grow_grad2d",
        "psnr",
        "ssim",
        "lpips",
        "ellipse_time",
        "num_GS"
    ]
    
    with open(output_csv, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=header)
        writer.writeheader()
        for item in results:
            row = {key: item.get(key, "") for key in header}
            writer.writerow(row)
    
    print(f"Analysis complete. Data saved to {output_csv}")
    
    # Write grouped CSV.
    grouped_csv = "grouped_results.csv"
    group_and_write_csv(results, grouped_csv, known_prefixes=[prefix, "bicycle_4_FULL"])
    print(f"Grouped CSV saved to {grouped_csv}")

def main():
    parser = argparse.ArgumentParser(
        description="Analyze student subdirectories for stats and training parameters from val_step29999.json and add teacher reference cases."
    )
    parser.add_argument('--students_dir', type=str, default='../gsplat_students_v6', 
                        help="Path to the directory containing student subdirectories")
    parser.add_argument('--output_csv', type=str, default='reduced_results.csv', 
                        help="Path to output CSV file (default: reduced_results.csv)")
    parser.add_argument('--prefix', type=str, default='bicycle_4_SMALL', 
                        help="Prefix used in subdirectory naming (default: 'bicycle_4_SMALL')")
    
    args = parser.parse_args()
    analyze_students_dir(args.students_dir, args.output_csv, prefix=args.prefix)

if __name__ == "__main__":
    main()
